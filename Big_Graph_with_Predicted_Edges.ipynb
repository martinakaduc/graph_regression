{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50da5afa-487c-4a28-aef0-de23c91a3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util \n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Optional, Callable, Tuple, Union\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing, NNConv, CGConv, GINEConv\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import reset, zeros\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.typing import Adj, OptPairTensor, OptTensor, Size\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader, ClusterData, ClusterLoader\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "DATA_FOLDER = \"./data\"\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic=True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5dfe610-633f-45f8-a976-8fed926bdef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_bigLITTLE_graph(data_folder, no_duplicate=False, unobserved=0.0, unobserved_edge=0.0):\n",
    "    list_files = os.listdir(data_folder)\n",
    "    list_files = list(filter(lambda x: x.endswith(\".pkl\"), list_files))\n",
    "    list_labels = pickle.load(open(\"labels.pkl\", \"rb\"))\n",
    "    \n",
    "    dict_graph_size = defaultdict(lambda: [])\n",
    "    set_cycle_size = set([])\n",
    "    set_branch_size = set([])\n",
    "    \n",
    "    # Directed or Undirected? Edge weights?\n",
    "    bigLITTLE_graph = nx.DiGraph()\n",
    "    gid = 0\n",
    "    \n",
    "    for gf in list_files:\n",
    "        idx, cycle_size, branch_size, _ = gf.split(\"_\")\n",
    "        cycle_size = int(cycle_size)\n",
    "        branch_size = int(branch_size)\n",
    "        \n",
    "        if no_duplicate and len(dict_graph_size[(cycle_size, branch_size)]) > 0:\n",
    "            continue\n",
    "            \n",
    "        # graph = nx.read_gpickle(os.path.join(DATA_FOLDER, gf))\n",
    "        \n",
    "        # For testing\n",
    "        # if cycle_size > 5 or branch_size > 2:\n",
    "        #     continue\n",
    "        \n",
    "        bigLITTLE_graph.add_node(gid, features=[float(cycle_size), float(branch_size)], label=list_labels[int(idx)])\n",
    "        dict_graph_size[(cycle_size, branch_size)].append(gid)\n",
    "        \n",
    "        set_cycle_size.add(cycle_size)\n",
    "        set_branch_size.add(branch_size)\n",
    "        gid += 1\n",
    "    # print(dict_graph_size)\n",
    "    # Undirected assumption?\n",
    "    # Same cycle_size & branch_size ==> Edge: [0,0]\n",
    "    # for k, items in dict_graph_size.items():\n",
    "    #     for item_idx1 in range(len(items)):\n",
    "    #         for item_idx2 in range(item_idx1+1, len(items)):\n",
    "    #             bigLITTLE_graph.add_edge(items[item_idx1], items[item_idx2], e=[0,0])\n",
    "                \n",
    "    # Same cycle size & Different branch size ==> Edge: [0, 1]\n",
    "    # Filter lists of same cycle_size\n",
    "    for cs in set_cycle_size:\n",
    "        list_same_cycle_size = list(filter(lambda x: x[0]==cs, dict_graph_size.keys()))\n",
    "        list_same_cycle_size = list(sorted(list_same_cycle_size, key=lambda x:x[1]))\n",
    "        # print(list_same_cycle_size)\n",
    "        for bs_idx1 in range(len(list_same_cycle_size)-1):\n",
    "            bs_idx2 = bs_idx1 + 1\n",
    "            key_cb1 = list_same_cycle_size[bs_idx1] # e.g. (3, 1)\n",
    "            key_cb2 = list_same_cycle_size[bs_idx2] # e.g. (3, 2)\n",
    "\n",
    "            for gid1 in dict_graph_size[key_cb1]:\n",
    "                for gid2 in dict_graph_size[key_cb2]:\n",
    "                    # print(key_cb2[1]-key_cb1[1])\n",
    "                    bigLITTLE_graph.add_edge(gid1, gid2, e=[0,1])\n",
    "                    bigLITTLE_graph.add_edge(gid2, gid1, e=[0,-1])\n",
    "                        \n",
    "    # Different cycle size & Same branch size ==> Edge: [1, 0]\n",
    "    for bs in set_branch_size:\n",
    "        list_same_branch_size = list(filter(lambda x: x[1]==bs, dict_graph_size.keys()))\n",
    "        list_same_branch_size = list(sorted(list_same_branch_size, key=lambda x:x[0]))\n",
    "        \n",
    "        for cs_idx1 in range(len(list_same_branch_size)-1):\n",
    "            cs_idx2 = cs_idx1 + 1\n",
    "            key_cb1 = list_same_branch_size[cs_idx1] # e.g. (3, 1)\n",
    "            key_cb2 = list_same_branch_size[cs_idx2] # e.g. (4, 1)\n",
    "\n",
    "            for gid1 in dict_graph_size[key_cb1]:\n",
    "                for gid2 in dict_graph_size[key_cb2]:\n",
    "                    bigLITTLE_graph.add_edge(gid1, gid2, e=[1,0])\n",
    "                    bigLITTLE_graph.add_edge(gid2, gid1, e=[-1,0])\n",
    "                    \n",
    "    \n",
    "    # Add all other edge as [0,0]\n",
    "    list_nodes = list(bigLITTLE_graph.nodes)\n",
    "    for i in range(len(list_nodes)):\n",
    "        for j in range(i, len(list_nodes)):\n",
    "            nid_i = list_nodes[i]\n",
    "            nid_j = list_nodes[j]\n",
    "            if not bigLITTLE_graph.has_edge(nid_i, nid_j):\n",
    "                bigLITTLE_graph.add_edge(gid1, gid2, e=[0,0])\n",
    "                bigLITTLE_graph.add_edge(gid2, gid1, e=[0,0])\n",
    "    \n",
    "    if unobserved > 0:\n",
    "        unobserved_node_idxs = np.random.choice(list(range(bigLITTLE_graph.number_of_nodes())), \n",
    "                                size=int(unobserved*bigLITTLE_graph.number_of_nodes()), \n",
    "                                replace=False)\n",
    "    else:\n",
    "        unobserved_node_idxs = None\n",
    "    \n",
    "    if unobserved_edge > 0:\n",
    "        # Remove unobserved edges\n",
    "        num_edge_to_remove = int(bigLITTLE_graph.number_of_edges() * unobserved_edge)\n",
    "            \n",
    "        list_edges = np.array([list(e) for e in bigLITTLE_graph.edges])\n",
    "        list_unique_edges = list_edges[list_edges[:,1] >= list_edges[:,0]]\n",
    "        edge_idxs = np.random.choice([0,1], \n",
    "                                size=list_unique_edges.shape[0], \n",
    "                                p=[unobserved_edge, 1-unobserved_edge])\n",
    "        list_remove_edges = list_unique_edges[edge_idxs==0]\n",
    "        \n",
    "        for edge in list_remove_edges:\n",
    "            u, v = edge\n",
    "            bigLITTLE_graph.remove_edge(u,v)\n",
    "            bigLITTLE_graph.remove_edge(v,u)\n",
    "    \n",
    "    return bigLITTLE_graph, unobserved_node_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ee263d8-fc0b-4989-9773-6748d8c6289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_color(e):\n",
    "    if e == [0,1]:\n",
    "        return \"black\"\n",
    "    elif e == [0,-1]:\n",
    "        return \"red\"\n",
    "    elif e == [1,0]:\n",
    "        return \"green\"\n",
    "    elif e == [-1,0]:\n",
    "        return \"blue\"\n",
    "    else:\n",
    "        return \"yellow\"\n",
    "    \n",
    "def draw_graph(graph):\n",
    "    nodeLabels = {nid:graph.nodes[nid][\"label\"] for nid in graph.nodes}\n",
    "    nodeColors = \"grey\"\n",
    "    edgeColor = [get_edge_color(graph.edges[eid][\"e\"])for eid in graph.edges]\n",
    "\n",
    "    nx.draw(graph, nx.kamada_kawai_layout(graph), edge_color=edgeColor, width=1, linewidths=0.1,\n",
    "              node_size=500, node_color=nodeColors, alpha=0.9,\n",
    "              labels=nodeLabels)\n",
    "    \n",
    "def transform_func(graph, device):\n",
    "    graph.x = graph.x.to(device)\n",
    "    graph.x_lp = graph.x_lp.to(device)\n",
    "    graph.y = graph.label.to(device)\n",
    "    graph.edge_attr = graph.edge_attr.to(device)\n",
    "    graph.edge_index = graph.edge_index.to(device)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7af08f0b-0550-4df0-84e4-497b6d13f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrickyAggregation(Aggregation):\n",
    "    '''\n",
    "    Aggregation for the tricky graph\n",
    "    This class is used to aggregate the top-k node features of the tricky graph by mean function\n",
    "    '''\n",
    "    def __init__(self, k=2):\n",
    "        super(TrickyAggregation, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
    "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
    "                dim: int = -2) -> Tensor:\n",
    "        \"\"\"\n",
    "        Get top-k node features by mean function among indexed nodes.\n",
    "        Using Optimal Transport to get the top-k node features.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class TrickyNNConv(MessagePassing):\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, aggr: str = 'add',\n",
    "                 root_weight: bool = True, bias: bool = True, **kwargs):\n",
    "\n",
    "        if aggr == 'tricky':\n",
    "            kwargs['aggr'] = TrickyAggregation()\n",
    "        else:\n",
    "            kwargs['aggr'] = aggr\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.root_weight = root_weight\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.nn = Linear(out_channels, out_channels, bias=False, weight_initializer='uniform')\n",
    "        if root_weight:\n",
    "            self.lin = Linear(in_channels[1], out_channels, bias=False,\n",
    "                              weight_initializer='uniform')\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.etn = Linear(out_channels, 1, bias=False,\n",
    "                              weight_initializer='uniform')\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "        if self.root_weight:\n",
    "            self.lin.reset_parameters()\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        x_r = self.lin(x)\n",
    "\n",
    "        # propagate_type: (x: OptTensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x_r, edge_attr=edge_attr, size=size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        # weight = self.nn(edge_attr)\n",
    "        weight = edge_attr.view(-1, self.out_channels)\n",
    "        # Graph 1 -- 1 node -- Graph 2\n",
    "        # (W*e) * (W*(We+x))\n",
    "        return self.etn(weight) * self.nn(weight + x_j).squeeze(1) # torch.matmul(x_j.unsqueeze(1), weight).squeeze(1)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, aggr={self.aggr}, nn={self.nn})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f38d362-f969-4f91-bb16-0ee74f3183e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, node_channels, edge_channels, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.edge_embed = nn.Sequential(\n",
    "                nn.Linear(edge_channels, hidden_channels, bias=True),\n",
    "                # nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "            )\n",
    "        \n",
    "        # self.node_embed = nn.Linear(node_channels, hidden_channels, bias=False)\n",
    "        # self.conv1 = CGConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.conv1 = TrickyNNConv(node_channels, hidden_channels, aggr=\"mean\")\n",
    "        self.conv1_e = nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels, bias=True),\n",
    "                # nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "            )\n",
    "        \n",
    "        self.conv2 = TrickyNNConv(hidden_channels, hidden_channels, aggr=\"mean\")\n",
    "        \n",
    "        # self.conv1 = GINEConv(\n",
    "        #     nn=nn.Sequential(nn.Linear(hidden_channels, node_channels*hidden_channels)),\n",
    "        #     edge_dim=hidden_channels\n",
    "        # )\n",
    "        \n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "        self.lin2 = nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "        self.lin3 = nn.Linear(hidden_channels, 1, bias=True)\n",
    "        \n",
    "        # Max aggregating, Top-k mean, sorted + weighted sum\n",
    "        # Increase GNN layers\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \n",
    "        '''\n",
    "        edge_attr: batch_size * 2\n",
    "        '''\n",
    "        # 1. Obtain node embeddings\n",
    "        # x = self.node_embed(x)\n",
    "        e = self.edge_embed(edge_attr)\n",
    "        \n",
    "        z = self.conv1(x=x, edge_index=edge_index, edge_attr=e)\n",
    "        z = z.relu()\n",
    "        \n",
    "        e = self.conv1_e(e)#.relu()\n",
    "        z = self.conv2(x=z, edge_index=edge_index, edge_attr=e)\n",
    "        z = z.relu()\n",
    "        \n",
    "        # 2. Apply a final classifier\n",
    "        z = F.dropout(z, p=0.1, training=True)\n",
    "        \n",
    "        z = self.lin1(z)\n",
    "        z = z.relu()\n",
    "        z = self.lin2(z)\n",
    "        z = z.relu()\n",
    "        \n",
    "        z = self.lin3(z)\n",
    "        z = torch.sigmoid(z) * 110\n",
    "        \n",
    "        return z\n",
    "    \n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, node_channels, edge_channels, hidden_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        \n",
    "        self.node_embed = nn.Linear(node_channels, hidden_channels, bias=False)\n",
    "        \n",
    "        self.lin1 = nn.Linear(node_channels, hidden_channels, bias=True)\n",
    "        self.lin2 = nn.Linear(hidden_channels, edge_channels, bias=True)\n",
    "\n",
    "    def forward(self, x, observed_edge_index, batch):\n",
    "        \n",
    "        '''\n",
    "        x: (num_nodes, 2)\n",
    "        observed_edge_nid: (num_observed_edge, 2)\n",
    "        '''\n",
    "        # 1. Obtain node embeddings\n",
    "        z = self.node_embed(x)\n",
    "        z = z.relu()\n",
    "        \n",
    "        # 2. Apply a final classifier\n",
    "        # z = F.dropout(z, p=0.1, training=True)\n",
    "        \n",
    "        # observed_edge_nid: [(1,2), (3,4)] ==> (x[1] <-> x[2])\n",
    "        # (40.1 30.1) ==> e:[1,0]\n",
    "        # (50.1 70.1) ==> e:[0,0]\n",
    "        head = x[observed_edge_index[1]]\n",
    "        tail = x[observed_edge_index[0]]\n",
    "        \n",
    "        e = head - tail\n",
    "        e = self.lin1(e)\n",
    "        e = self.lin2(e)\n",
    "        return e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9a744a9-9e8b-41c9-8c51-283968254d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointModel(nn.Module):\n",
    "    def __init__(self, node_channels, edge_channels, hidden_channels):\n",
    "        super(JointModel, self).__init__()\n",
    "        \n",
    "        #=============================\n",
    "        # LINK PREDICTOR\n",
    "        #=============================\n",
    "        self.link_predictor = LinkPredictor(node_channels=node_channels, edge_channels=edge_channels, \n",
    "                    hidden_channels=hidden_channels)\n",
    "        lp_parameters = filter(lambda p: p.requires_grad, self.link_predictor.parameters())\n",
    "        lp_params = sum([np.prod(p.size()) for p in lp_parameters])\n",
    "        print(self.link_predictor)\n",
    "        print(\"Number of LP parameters: \", lp_params)\n",
    "\n",
    "\n",
    "        #=============================\n",
    "        # NODE REGRESSION\n",
    "        #=============================\n",
    "        self.gnn = GNN(node_channels=node_channels, edge_channels=edge_channels, \n",
    "                    hidden_channels=hidden_channels)\n",
    "        gnn_parameters = filter(lambda p: p.requires_grad, self.gnn.parameters())\n",
    "        gnn_params = sum([np.prod(p.size()) for p in gnn_parameters])\n",
    "        print(self.gnn)\n",
    "        print(\"Number of GNN parameters: \", gnn_params)\n",
    "\n",
    "    def forward(self, x, x_lp, edge_index, batch, edge_attr=None):\n",
    "        predicted_edge_attr = self.link_predictor(x_lp, edge_index, batch)\n",
    "        # Stil remaining edge [0,0]\n",
    "        \n",
    "        if self.training and edge_attr is not None:\n",
    "            # TEACHER FORCING\n",
    "            node_y = self.gnn(x, edge_index, edge_attr, batch)\n",
    "        else:\n",
    "            # NON-TEACHER FORCING\n",
    "            node_y = self.gnn(x, edge_index, predicted_edge_attr, batch)\n",
    "            \n",
    "        return predicted_edge_attr, node_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd951571-58f4-4322-8afd-05ec354341e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, random_mask=0, observed_idxs=None, criterion=None, optimizer=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Iterate in batches over the training dataset\n",
    "    for data in loader:\n",
    "        edge_attr, node_y = model(data.x, data.x_lp, data.edge_index, data.batch, edge_attr=data.edge_attr)\n",
    "        \n",
    "        # Compute the loss for link predictor\n",
    "        lp_loss = criterion(\n",
    "            edge_attr, \n",
    "            data.edge_attr\n",
    "        )\n",
    "        # Compute the loss\n",
    "        node_loss = criterion(\n",
    "            node_y[observed_idxs], \n",
    "            data.y[observed_idxs].view(-1, 1)\n",
    "        )\n",
    "        \n",
    "        total_loss += lp_loss + node_loss\n",
    "        total_loss.backward(); \n",
    "        optimizer.step(); optimizer.zero_grad(); steps += 1\n",
    "\n",
    "    return total_loss / steps\n",
    "\n",
    "def test(model, loader, mc_dropout_sample=100, criterion=None):\n",
    "    model.eval()\n",
    "    mse = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Iterate in batches over the training/test dataset\n",
    "    for data in loader:\n",
    "        node_ys = []\n",
    "        edge_attrs = []\n",
    "        \n",
    "        for _ in range(mc_dropout_sample):\n",
    "            # NON-TEACHER FORCING\n",
    "            edge_attr, node_y = model(data.x, data.x_lp, data.edge_index, data.batch)\n",
    "            \n",
    "            edge_attrs.append(edge_attr)\n",
    "            node_ys.append(node_y)\n",
    "        edge_attrs = torch.stack(edge_attrs).detach().cpu()\n",
    "        node_ys = torch.stack(node_ys).detach().cpu()\n",
    "        \n",
    "        # Check against ground-truth labels\n",
    "        mse += criterion(edge_attrs.mean(0), data.edge_attr.detach().cpu()) \\\n",
    "            + criterion(node_ys.mean(0), data.y.view(-1, 1).detach().cpu())\n",
    "        steps += 1\n",
    "        \n",
    "        # out_std = out.std(0)\n",
    "    return mse / steps  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b4886e-f0bd-4193-a35b-df0be64f47de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n",
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "generate_data = True\n",
    "unobserved_fraction = 0.2\n",
    "device_data = \"cuda:1\"\n",
    "\n",
    "if generate_data:\n",
    "    bL_graph_train, unobserved_idxs = construct_bigLITTLE_graph(DATA_FOLDER, \n",
    "                                            unobserved=unobserved_fraction,\n",
    "                                            unobserved_edge=unobserved_fraction)\n",
    "    observed_idxs = list(set(bL_graph_train.nodes) - set(unobserved_idxs))\n",
    "    # draw_graph(bL_graph_train)\n",
    "    \n",
    "    data_train = from_networkx(bL_graph_train)\n",
    "    data_train.x = torch.ones(data_train.label.shape[0], 2).type(torch.FloatTensor)\n",
    "    data_train.x[:,1] = 0\n",
    "    data_train.x_lp = data_train.features.type(torch.FloatTensor)\n",
    "    data_train.edge_attr = data_train.e.type(torch.FloatTensor)\n",
    "    data_train = transform_func(data_train, device_data)\n",
    "    \n",
    "    c_data_train = ClusterData(data_train, num_parts=1)\n",
    "    train_loader = ClusterLoader(c_data_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    bL_graph_test, _ = construct_bigLITTLE_graph(DATA_FOLDER)\n",
    "    # draw_graph(bL_graph)\n",
    "    \n",
    "    data_test = from_networkx(bL_graph_test)\n",
    "    data_test.x = torch.ones(data_test.label.shape[0], 2).type(torch.FloatTensor)\n",
    "    data_test.x[:,1] = 0\n",
    "    data_test.x_lp = data_test.features.type(torch.FloatTensor)\n",
    "    data_test.edge_attr = data_test.e.type(torch.FloatTensor)\n",
    "    data_test = transform_func(data_test, device_data)\n",
    "    \n",
    "    c_data_test = ClusterData(data_test, num_parts=1)\n",
    "    test_loader = ClusterLoader(c_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e3d9020-66f8-4afb-bd45-f0481484b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Test unconnected graph\n",
    "neighbors_list = []\n",
    "for nid in bL_graph_train.nodes:\n",
    "    node_neighbors = bL_graph_train.neighbors(nid)\n",
    "    node_observed_neighbors = set(node_neighbors) - set(unobserved_idxs)\n",
    "    node_unobserved_neighbors = set(node_neighbors) - set(node_observed_neighbors)\n",
    "    neighbors_list.append([nid, len(node_observed_neighbors), len(node_unobserved_neighbors)])\n",
    "# print(neighbors_list)\n",
    "neighbors_list = np.array(neighbors_list)\n",
    "non_neighbor_nids = np.where(neighbors_list[:,1] == 0)[0]\n",
    "print(non_neighbor_nids)\n",
    "assert non_neighbor_nids.shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e449bb3-11ac-4115-95d2-c734ede9d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mse = 1e10\n",
    "min_epoch = 0\n",
    "epochs = 100000\n",
    "lr = 0.0005\n",
    "device = device_data\n",
    "hidden_channels = 64\n",
    "\n",
    "\n",
    "model = JointModel(node_channels=2, edge_channels=2, \n",
    "            hidden_channels=hidden_channels).to(device)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(model)\n",
    "print(\"Number of parameters: \", params)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_mse = train(model, train_loader, observed_idxs=observed_idxs, criterion=criterion, optimizer=optimizer)\n",
    "    \n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        clear_output(wait=True)\n",
    "        test_mse = test(model, test_loader, criterion=criterion)\n",
    "        if test_mse < min_mse:\n",
    "            min_mse = test_mse\n",
    "            min_epoch = epoch\n",
    "        print(f'Epoch: {epoch+1:03d}, Train MAE: {train_mse:.4f},',\n",
    "              f'Test MAE: {test_mse:.4f}, Min MAE: {min_mse:.4f}')\n",
    "    else: test_mse = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78a9cd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 14 09:19:40 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    73W / 400W |  18823MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    61W / 400W |      2MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:44:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    60W / 400W |      2MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM...  On   | 00000000:4A:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    64W / 400W |      2MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM...  On   | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    66W / 400W |      2MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    64W / 400W |      2MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM...  On   | 00000000:C0:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    60W / 400W |      2MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM...  On   | 00000000:C3:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    61W / 400W |      2MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2253193      C   ...uong/geometric/bin/python    18821MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293591b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "geometric"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
