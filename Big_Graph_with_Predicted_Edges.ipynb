{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util \n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Optional, Callable, Tuple, Union\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing, NNConv, CGConv, GINEConv\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import reset, zeros, glorot\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.typing import Adj, OptPairTensor, OptTensor, Size\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader, ClusterData, ClusterLoader\n",
    "from torch_geometric.utils import from_networkx, to_networkx, add_self_loops, remove_self_loops, softmax\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PROJECT_FOLDER = \"/dfs/user/sttruong/DucWorkspace/graph_regression\"\n",
    "DATA_FOLDER = PROJECT_FOLDER + \"/data\"\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic=True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_bigLITTLE_graph(data_folder, no_duplicate=False, unobserved=0.0, unobserved_edge=0.0):\n",
    "    list_files = os.listdir(data_folder)\n",
    "    list_files = list(filter(lambda x: x.endswith(\".pkl\"), list_files))\n",
    "    list_labels = pickle.load(open(PROJECT_FOLDER + \"/labels.pkl\", \"rb\"))\n",
    "    \n",
    "    dict_graph_size = defaultdict(lambda: [])\n",
    "    set_cycle_size = set([])\n",
    "    set_branch_size = set([])\n",
    "    \n",
    "    # Directed or Undirected? Edge weights?\n",
    "    bigLITTLE_graph = nx.DiGraph()\n",
    "    gid = 0\n",
    "    \n",
    "    for gf in list_files:\n",
    "        idx, cycle_size, branch_size, _ = gf.split(\"_\")\n",
    "        cycle_size = int(cycle_size)\n",
    "        branch_size = int(branch_size)\n",
    "        \n",
    "        if no_duplicate and len(dict_graph_size[(cycle_size, branch_size)]) > 0:\n",
    "            continue\n",
    "            \n",
    "        # graph = nx.read_gpickle(os.path.join(DATA_FOLDER, gf))\n",
    "        \n",
    "        # For testing\n",
    "        # if cycle_size > 5 or branch_size > 2:\n",
    "        #     continue\n",
    "        \n",
    "        bigLITTLE_graph.add_node(gid, features=[float(cycle_size), float(branch_size)], label=list_labels[int(idx)])\n",
    "        dict_graph_size[(cycle_size, branch_size)].append(gid)\n",
    "        \n",
    "        set_cycle_size.add(cycle_size)\n",
    "        set_branch_size.add(branch_size)\n",
    "        gid += 1\n",
    "    # print(dict_graph_size)\n",
    "    # Undirected assumption?\n",
    "    # Same cycle_size & branch_size ==> Edge: [0,0]\n",
    "    # for k, items in dict_graph_size.items():\n",
    "    #     for item_idx1 in range(len(items)):\n",
    "    #         for item_idx2 in range(item_idx1+1, len(items)):\n",
    "    #             bigLITTLE_graph.add_edge(items[item_idx1], items[item_idx2], e=[0,0])\n",
    "                \n",
    "    # Same cycle size & Different branch size ==> Edge: [0, 1]\n",
    "    # Filter lists of same cycle_size\n",
    "    for cs in set_cycle_size:\n",
    "        list_same_cycle_size = list(filter(lambda x: x[0]==cs, dict_graph_size.keys()))\n",
    "        list_same_cycle_size = list(sorted(list_same_cycle_size, key=lambda x:x[1]))\n",
    "        # print(list_same_cycle_size)\n",
    "        for bs_idx1 in range(len(list_same_cycle_size)-1):\n",
    "            bs_idx2 = bs_idx1 + 1\n",
    "            key_cb1 = list_same_cycle_size[bs_idx1] # e.g. (3, 1)\n",
    "            key_cb2 = list_same_cycle_size[bs_idx2] # e.g. (3, 2)\n",
    "\n",
    "            for gid1 in dict_graph_size[key_cb1]:\n",
    "                for gid2 in dict_graph_size[key_cb2]:\n",
    "                    # print(key_cb2[1]-key_cb1[1])\n",
    "                    bigLITTLE_graph.add_edge(gid1, gid2, e=[0,1])\n",
    "                    bigLITTLE_graph.add_edge(gid2, gid1, e=[0,-1])\n",
    "                        \n",
    "    # Different cycle size & Same branch size ==> Edge: [1, 0]\n",
    "    for bs in set_branch_size:\n",
    "        list_same_branch_size = list(filter(lambda x: x[1]==bs, dict_graph_size.keys()))\n",
    "        list_same_branch_size = list(sorted(list_same_branch_size, key=lambda x:x[0]))\n",
    "        \n",
    "        for cs_idx1 in range(len(list_same_branch_size)-1):\n",
    "            cs_idx2 = cs_idx1 + 1\n",
    "            key_cb1 = list_same_branch_size[cs_idx1] # e.g. (3, 1)\n",
    "            key_cb2 = list_same_branch_size[cs_idx2] # e.g. (4, 1)\n",
    "\n",
    "            for gid1 in dict_graph_size[key_cb1]:\n",
    "                for gid2 in dict_graph_size[key_cb2]:\n",
    "                    bigLITTLE_graph.add_edge(gid1, gid2, e=[1,0])\n",
    "                    bigLITTLE_graph.add_edge(gid2, gid1, e=[-1,0])\n",
    "                    \n",
    "    \n",
    "    # Add all other edge as [0,0]\n",
    "    list_nodes = list(bigLITTLE_graph.nodes)\n",
    "    for i in range(len(list_nodes)):\n",
    "        for j in range(i, len(list_nodes)):\n",
    "            nid_i = list_nodes[i]\n",
    "            nid_j = list_nodes[j]\n",
    "            if not bigLITTLE_graph.has_edge(nid_i, nid_j):\n",
    "                bigLITTLE_graph.add_edge(gid1, gid2, e=[0,0])\n",
    "                bigLITTLE_graph.add_edge(gid2, gid1, e=[0,0])\n",
    "    \n",
    "    if unobserved > 0:\n",
    "        unobserved_node_idxs = np.random.choice(list(range(bigLITTLE_graph.number_of_nodes())), \n",
    "                                size=int(unobserved*bigLITTLE_graph.number_of_nodes()), \n",
    "                                replace=False)\n",
    "    else:\n",
    "        unobserved_node_idxs = None\n",
    "    \n",
    "    if unobserved_edge > 0:\n",
    "        # Remove unobserved edges\n",
    "        num_edge_to_remove = int(bigLITTLE_graph.number_of_edges() * unobserved_edge)\n",
    "            \n",
    "        list_edges = np.array([list(e) for e in bigLITTLE_graph.edges])\n",
    "        list_unique_edges = list_edges[list_edges[:,1] >= list_edges[:,0]]\n",
    "        edge_idxs = np.random.choice([0,1], \n",
    "                                size=list_unique_edges.shape[0], \n",
    "                                p=[unobserved_edge, 1-unobserved_edge])\n",
    "        list_remove_edges = list_unique_edges[edge_idxs==0]\n",
    "        \n",
    "        for edge in list_remove_edges:\n",
    "            u, v = edge\n",
    "            bigLITTLE_graph.remove_edge(u,v)\n",
    "            bigLITTLE_graph.remove_edge(v,u)\n",
    "    \n",
    "    return bigLITTLE_graph, unobserved_node_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_color(e):\n",
    "    if e == [0,1]:\n",
    "        return \"black\"\n",
    "    elif e == [0,-1]:\n",
    "        return \"red\"\n",
    "    elif e == [1,0]:\n",
    "        return \"green\"\n",
    "    elif e == [-1,0]:\n",
    "        return \"blue\"\n",
    "    else:\n",
    "        return \"yellow\"\n",
    "    \n",
    "def draw_graph(graph):\n",
    "    nodeLabels = {nid:graph.nodes[nid][\"label\"] for nid in graph.nodes}\n",
    "    nodeColors = \"grey\"\n",
    "    edgeColor = [get_edge_color(graph.edges[eid][\"e\"])for eid in graph.edges]\n",
    "\n",
    "    nx.draw(graph, nx.kamada_kawai_layout(graph), edge_color=edgeColor, width=1, linewidths=0.1,\n",
    "              node_size=500, node_color=nodeColors, alpha=0.9,\n",
    "              labels=nodeLabels)\n",
    "    \n",
    "def transform_func(graph, device):\n",
    "    graph.x = graph.x.to(device)\n",
    "    graph.x_lp = graph.x_lp.to(device)\n",
    "    graph.y = graph.label.to(device)\n",
    "    graph.edge_attr = graph.edge_attr.to(device)\n",
    "    graph.edge_index = graph.edge_index.to(device)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrickyAggregation(Aggregation):\n",
    "    '''\n",
    "    Aggregation for the tricky graph\n",
    "    This class is used to aggregate the top-k node features of the tricky graph by mean function\n",
    "    '''\n",
    "    def __init__(self, k=2):\n",
    "        super(TrickyAggregation, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
    "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
    "                dim: int = -2) -> Tensor:\n",
    "        \"\"\"\n",
    "        Get top-k node features by mean function among indexed nodes.\n",
    "        Using Optimal Transport to get the top-k node features.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class TrickyNNConv(MessagePassing):\n",
    "    def __init__(self, \n",
    "                 in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, aggr: str = 'mean', \n",
    "                 edge_dim: Optional[int] = None,\n",
    "                 add_self_loops: bool = False, \n",
    "                 negative_slope: float = 0.2, \n",
    "                 fill_value: Union[float, Tensor, str] = 'mean',\n",
    "                 dropout: float = 0.0,\n",
    "                 bias: bool = False, **kwargs):\n",
    "\n",
    "        if aggr == 'tricky':\n",
    "            kwargs['aggr'] = TrickyAggregation()\n",
    "        else:\n",
    "            kwargs['aggr'] = aggr\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.edge_dim = edge_dim\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.negative_slope = negative_slope\n",
    "        self.fill_value = fill_value\n",
    "        self.dropout = dropout\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        \n",
    "        if isinstance(in_channels, int):\n",
    "            self.lin_src = Linear(in_channels, out_channels,\n",
    "                                  bias=False, weight_initializer='glorot')\n",
    "            self.lin_dst = self.lin_src\n",
    "        else:\n",
    "            self.lin_src = Linear(in_channels[0], out_channels, False,\n",
    "                                  weight_initializer='glorot')\n",
    "            self.lin_dst = Linear(in_channels[1], out_channels, False,\n",
    "                                  weight_initializer='glorot')\n",
    "            \n",
    "        # The learnable parameters to compute attention coefficients:\n",
    "        # self.att_src = Parameter(torch.Tensor(1, out_channels))\n",
    "        # self.att_dst = Parameter(torch.Tensor(1, out_channels))\n",
    "\n",
    "        if edge_dim is not None:\n",
    "            self.att_edge = Parameter(torch.Tensor(1, out_channels))\n",
    "        else:\n",
    "            self.register_parameter('att_edge', None)    \n",
    "        \n",
    "        self.nn = Linear(out_channels, out_channels, bias=False, weight_initializer='uniform')\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "        \n",
    "        self.lin_src.reset_parameters()\n",
    "        self.lin_dst.reset_parameters()\n",
    "        \n",
    "        # glorot(self.att_src)\n",
    "        # glorot(self.att_dst)\n",
    "        glorot(self.att_edge)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        if isinstance(x, Tensor):\n",
    "            assert x.dim() == 2, \"Static graphs not supported\"\n",
    "            x_src = x_dst = self.lin_src(x)\n",
    "        else:  # Tuple of source and target node features:\n",
    "            x_src, x_dst = x\n",
    "            assert x_src.dim() == 2, \"Static graphs not supported\"\n",
    "            x_src = self.lin_src(x_src)\n",
    "            if x_dst is not None:\n",
    "                x_dst = self.lin_dst(x_dst)\n",
    "                \n",
    "        x = (x_src, x_dst)\n",
    "\n",
    "        # Next, we compute node-level attention coefficients, both for source\n",
    "        # and target nodes (if present):\n",
    "        # alpha_src = (x_src * self.att_src).sum(dim=-1).view(1, -1)\n",
    "        # alpha_dst = None if x_dst is None else (x_dst * self.att_dst).sum(-1).view(1, -1)\n",
    "        # alpha = (alpha_src, alpha_dst)\n",
    "        alpha = x\n",
    "        \n",
    "        if self.add_self_loops:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                # We only want to add self-loops for nodes that appear both as\n",
    "                # source and target nodes:\n",
    "                num_nodes = x_src.size(0)\n",
    "                if x_dst is not None:\n",
    "                    num_nodes = min(num_nodes, x_dst.size(0))\n",
    "                num_nodes = min(size) if size is not None else num_nodes\n",
    "                edge_index, edge_attr = remove_self_loops(\n",
    "                    edge_index, edge_attr)\n",
    "                edge_index, edge_attr = add_self_loops(\n",
    "                    edge_index, edge_attr, fill_value=self.fill_value,\n",
    "                    num_nodes=num_nodes)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                if self.edge_dim is None:\n",
    "                    edge_index = set_diag(edge_index)\n",
    "                else:\n",
    "                    raise NotImplementedError(\n",
    "                        \"The usage of 'edge_attr' and 'add_self_loops' \"\n",
    "                        \"simultaneously is currently not yet supported for \"\n",
    "                        \"'edge_index' in a 'SparseTensor' form\")\n",
    "\n",
    "        # edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\n",
    "        alpha = self.edge_updater(edge_index, alpha=alpha, edge_attr=edge_attr)\n",
    "        \n",
    "        # propagate_type: (x: OptTensor, alpha: Tensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, alpha=alpha, size=size)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def edge_update(self, alpha_j: Tensor, alpha_i: OptTensor,\n",
    "                    edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n",
    "                    size_i: Optional[int]) -> Tensor:\n",
    "        # Given edge-level attention coefficients for source and target nodes,\n",
    "        # we simply need to sum them up to \"emulate\" concatenation:\n",
    "        # alpha = alpha_j if alpha_i is None else alpha_j + alpha_i\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            if edge_attr.dim() == 1:\n",
    "                edge_attr = edge_attr.view(-1, 1)\n",
    "            edge_attr = edge_attr.view(-1, self.out_channels)\n",
    "            alpha_edge = (edge_attr * self.att_edge).sum(dim=-1)\n",
    "            alpha = alpha_edge ** 2\n",
    "\n",
    "        # alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        # alpha = (alpha * self.att_edge).sum(dim=-1)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return alpha\n",
    "    \n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor, alpha: Tensor) -> Tensor:\n",
    "        weight = edge_attr.view(-1, self.out_channels)\n",
    "        return alpha.unsqueeze(-1) * self.nn(weight + x_j).squeeze(1) # torch.matmul(x_j.unsqueeze(1), weight).squeeze(1)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, aggr={self.aggr}, nn={self.nn})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, node_channels, edge_channels, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.edge_embed = nn.Sequential(\n",
    "                Linear(edge_channels, hidden_channels, bias=False,\n",
    "                                   weight_initializer='glorot')\n",
    "                # nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "            )\n",
    "        \n",
    "        self.conv1 = TrickyNNConv(node_channels, hidden_channels, aggr=\"add\", edge_dim=hidden_channels)\n",
    "        self.conv1_e = nn.Sequential(\n",
    "                Linear(hidden_channels, hidden_channels, bias=False,\n",
    "                                   weight_initializer='glorot')\n",
    "                # nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "            )\n",
    "        \n",
    "        self.conv2 = TrickyNNConv(hidden_channels, hidden_channels, aggr=\"add\", edge_dim=hidden_channels)\n",
    "        \n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "        self.lin2 = nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "        self.lin3 = nn.Linear(hidden_channels, 1, bias=True)\n",
    "        \n",
    "        # Max aggregating, Top-k mean, sorted + weighted sum\n",
    "        # Increase GNN layers\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \n",
    "        '''\n",
    "        edge_attr: batch_size * 2\n",
    "        '''\n",
    "        # 1. Obtain node embeddings\n",
    "        # x = self.node_embed(x)\n",
    "        e = self.edge_embed(edge_attr)\n",
    "        \n",
    "        z = self.conv1(x=x, edge_index=edge_index, edge_attr=e)\n",
    "        z = z.relu()\n",
    "        \n",
    "        e = self.conv1_e(e)#.relu()\n",
    "        z = self.conv2(x=z, edge_index=edge_index, edge_attr=e)\n",
    "        z = z.relu()\n",
    "        \n",
    "        # 2. Apply a final classifier\n",
    "        z = F.dropout(z, p=0.1, training=True)\n",
    "        \n",
    "        z = self.lin1(z)\n",
    "        z = z.relu()\n",
    "        z = self.lin2(z)\n",
    "        z = z.relu()\n",
    "        \n",
    "        z = self.lin3(z)\n",
    "        z = torch.sigmoid(z) * 110\n",
    "        \n",
    "        return z\n",
    "    \n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, node_channels, edge_channels, hidden_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        \n",
    "        self.node_embed = nn.Linear(node_channels, hidden_channels, bias=False)\n",
    "        \n",
    "        self.lin1 = nn.Linear(node_channels, hidden_channels, bias=True)\n",
    "        self.lin2 = nn.Linear(hidden_channels, edge_channels, bias=True)\n",
    "\n",
    "    def forward(self, x, observed_edge_index, batch):\n",
    "        \n",
    "        '''\n",
    "        x: (num_nodes, 2)\n",
    "        observed_edge_nid: (num_observed_edge, 2)\n",
    "        '''\n",
    "        # 1. Obtain node embeddings\n",
    "        z = self.node_embed(x)\n",
    "        z = z.relu()\n",
    "        \n",
    "        # 2. Apply a final classifier\n",
    "        # z = F.dropout(z, p=0.1, training=True)\n",
    "        \n",
    "        # observed_edge_nid: [(1,2), (3,4)] ==> (x[1] <-> x[2])\n",
    "        # (40.1 30.1) ==> e:[1,0]\n",
    "        # (50.1 70.1) ==> e:[0,0]\n",
    "        head = x[observed_edge_index[1]]\n",
    "        tail = x[observed_edge_index[0]]\n",
    "        \n",
    "        e = head - tail\n",
    "        e = self.lin1(e)\n",
    "        e = self.lin2(e)\n",
    "        return e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointModel(nn.Module):\n",
    "    def __init__(self, node_channels, edge_channels, hidden_channels):\n",
    "        super(JointModel, self).__init__()\n",
    "        \n",
    "        #=============================\n",
    "        # LINK PREDICTOR\n",
    "        #=============================\n",
    "        self.link_predictor = LinkPredictor(node_channels=node_channels, edge_channels=edge_channels, \n",
    "                    hidden_channels=hidden_channels)\n",
    "        lp_parameters = filter(lambda p: p.requires_grad, self.link_predictor.parameters())\n",
    "        lp_params = sum([np.prod(p.size()) for p in lp_parameters])\n",
    "        print(self.link_predictor)\n",
    "        print(\"Number of LP parameters: \", lp_params)\n",
    "\n",
    "\n",
    "        #=============================\n",
    "        # NODE REGRESSION\n",
    "        #=============================\n",
    "        self.gnn = GNN(node_channels=node_channels, edge_channels=edge_channels, \n",
    "                    hidden_channels=hidden_channels)\n",
    "        gnn_parameters = filter(lambda p: p.requires_grad, self.gnn.parameters())\n",
    "        gnn_params = sum([np.prod(p.size()) for p in gnn_parameters])\n",
    "        print(self.gnn)\n",
    "        print(\"Number of GNN parameters: \", gnn_params)\n",
    "\n",
    "    def forward(self, x, x_lp, edge_index, batch, edge_attr=None):\n",
    "        predicted_edge_attr = self.link_predictor(x_lp, edge_index, batch)\n",
    "        # Stil remaining edge [0,0]\n",
    "        \n",
    "        if self.training and edge_attr is not None:\n",
    "            # TEACHER FORCING\n",
    "            node_y = self.gnn(x, edge_index, edge_attr, batch)\n",
    "        else:\n",
    "            # NON-TEACHER FORCING\n",
    "            node_y = self.gnn(x, edge_index, predicted_edge_attr, batch)\n",
    "            \n",
    "        return predicted_edge_attr, node_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, random_mask=0, observed_idxs=None, criterion=None, optimizer=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Iterate in batches over the training dataset\n",
    "    for data in loader:\n",
    "        edge_attr, node_y = model(data.x, data.x_lp, data.edge_index, data.batch, edge_attr=data.edge_attr)\n",
    "        \n",
    "        # Compute the loss for link predictor\n",
    "        lp_loss = criterion(\n",
    "            edge_attr, \n",
    "            data.edge_attr\n",
    "        )\n",
    "        # Compute the loss\n",
    "        node_loss = criterion(\n",
    "            node_y[observed_idxs], \n",
    "            data.y[observed_idxs].view(-1, 1)\n",
    "        )\n",
    "        \n",
    "        total_loss += lp_loss + node_loss\n",
    "        total_loss.backward(); \n",
    "        optimizer.step(); optimizer.zero_grad(); steps += 1\n",
    "\n",
    "    return total_loss / steps\n",
    "\n",
    "def test(model, loader, mc_dropout_sample=100, criterion=None):\n",
    "    model.eval()\n",
    "    mse = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Iterate in batches over the training/test dataset\n",
    "    for data in loader:\n",
    "        node_ys = []\n",
    "        edge_attrs = []\n",
    "        \n",
    "        for _ in range(mc_dropout_sample):\n",
    "            # NON-TEACHER FORCING\n",
    "            edge_attr, node_y = model(data.x, data.x_lp, data.edge_index, data.batch)\n",
    "            \n",
    "            edge_attrs.append(edge_attr)\n",
    "            node_ys.append(node_y)\n",
    "        edge_attrs = torch.stack(edge_attrs).detach().cpu()\n",
    "        node_ys = torch.stack(node_ys).detach().cpu()\n",
    "        \n",
    "        # Check against ground-truth labels\n",
    "        mse += criterion(edge_attrs.mean(0), data.edge_attr.detach().cpu()) \\\n",
    "            + criterion(node_ys.mean(0), data.y.view(-1, 1).detach().cpu())\n",
    "        steps += 1\n",
    "        \n",
    "        # out_std = out.std(0)\n",
    "    return mse / steps  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n",
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "generate_data = True\n",
    "unobserved_fraction = 0.2\n",
    "device_data = \"cuda:7\"\n",
    "\n",
    "if generate_data:\n",
    "    bL_graph_train, unobserved_idxs = construct_bigLITTLE_graph(DATA_FOLDER, \n",
    "                                            unobserved=unobserved_fraction,\n",
    "                                            unobserved_edge=unobserved_fraction)\n",
    "    observed_idxs = list(set(bL_graph_train.nodes) - set(unobserved_idxs))\n",
    "    # draw_graph(bL_graph_train)\n",
    "    \n",
    "    data_train = from_networkx(bL_graph_train)\n",
    "    data_train.x = torch.ones(data_train.label.shape[0], 2).type(torch.FloatTensor)\n",
    "    data_train.x[:,1] = 0\n",
    "    data_train.x_lp = data_train.features.type(torch.FloatTensor)\n",
    "    data_train.edge_attr = data_train.e.type(torch.FloatTensor)\n",
    "    data_train = transform_func(data_train, device_data)\n",
    "    \n",
    "    c_data_train = ClusterData(data_train, num_parts=1)\n",
    "    train_loader = ClusterLoader(c_data_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    bL_graph_test, _ = construct_bigLITTLE_graph(DATA_FOLDER)\n",
    "    # draw_graph(bL_graph)\n",
    "    \n",
    "    data_test = from_networkx(bL_graph_test)\n",
    "    data_test.x = torch.ones(data_test.label.shape[0], 2).type(torch.FloatTensor)\n",
    "    data_test.x[:,1] = 0\n",
    "    data_test.x_lp = data_test.features.type(torch.FloatTensor)\n",
    "    data_test.edge_attr = data_test.e.type(torch.FloatTensor)\n",
    "    data_test = transform_func(data_test, device_data)\n",
    "    \n",
    "    c_data_test = ClusterData(data_test, num_parts=1)\n",
    "    test_loader = ClusterLoader(c_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Test unconnected graph\n",
    "neighbors_list = []\n",
    "for nid in bL_graph_train.nodes:\n",
    "    node_neighbors = bL_graph_train.neighbors(nid)\n",
    "    node_observed_neighbors = set(node_neighbors) - set(unobserved_idxs)\n",
    "    node_unobserved_neighbors = set(node_neighbors) - set(node_observed_neighbors)\n",
    "    neighbors_list.append([nid, len(node_observed_neighbors), len(node_unobserved_neighbors)])\n",
    "# print(neighbors_list)\n",
    "neighbors_list = np.array(neighbors_list)\n",
    "non_neighbor_nids = np.where(neighbors_list[:,1] == 0)[0]\n",
    "print(non_neighbor_nids)\n",
    "assert non_neighbor_nids.shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [37:29<00:00, 44.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100000, Train MAE: 0.0355, Test MAE: 0.4429, Min MAE: 0.2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_mse = 1e10\n",
    "min_epoch = 0\n",
    "epochs = 100000\n",
    "lr = 0.0005\n",
    "device = device_data\n",
    "hidden_channels = 64\n",
    "\n",
    "\n",
    "model = JointModel(node_channels=2, edge_channels=2, \n",
    "            hidden_channels=hidden_channels).to(device)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(model)\n",
    "print(\"Number of parameters: \", params)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_mse = train(model, train_loader, observed_idxs=observed_idxs, criterion=criterion, optimizer=optimizer)\n",
    "    \n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        clear_output(wait=True)\n",
    "        test_mse = test(model, test_loader, criterion=criterion)\n",
    "        if test_mse < min_mse:\n",
    "            min_mse = test_mse\n",
    "            min_epoch = epoch\n",
    "        print(f'Epoch: {epoch+1:03d}, Train MAE: {train_mse:.4f},',\n",
    "              f'Test MAE: {test_mse:.4f}, Min MAE: {min_mse:.4f}')\n",
    "    else: test_mse = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"/dfs/user/sttruong/DucWorkspace/graph_regression/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 22 19:57:08 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Quadro R...  On   | 00000000:43:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    57W / 250W |   7166MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA Quadro R...  On   | 00000000:44:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    57W / 250W |   2775MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA Quadro R...  On   | 00000000:45:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    59W / 250W |   4214MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA Quadro R...  On   | 00000000:46:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    61W / 250W |   4366MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA Quadro R...  On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    56W / 250W |  10438MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA Quadro R...  On   | 00000000:83:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    58W / 250W |   7953MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA Quadro R...  On   | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   33C    P8    14W / 250W |      3MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA Quadro R...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    59W / 250W |    692MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   8  NVIDIA Quadro R...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    58W / 250W |  27111MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   9  NVIDIA Quadro R...  On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    60W / 250W |  17804MiB / 45556MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3394      C   python                           1141MiB |\n",
      "|    0   N/A  N/A    120164      C   ...lin/miniconda3/bin/python     1667MiB |\n",
      "|    0   N/A  N/A    122047      C   ...lin/miniconda3/bin/python     1719MiB |\n",
      "|    0   N/A  N/A    127829      C   ...lin/miniconda3/bin/python     1667MiB |\n",
      "|    0   N/A  N/A    260487      C   .../envs/neuro/bin/python3.8      965MiB |\n",
      "|    1   N/A  N/A     67440      C   python                           1435MiB |\n",
      "|    2   N/A  N/A     10070      C   python                           1435MiB |\n",
      "|    2   N/A  N/A     72215      C   python                           1435MiB |\n",
      "|    4   N/A  N/A     31804      C   python                           2087MiB |\n",
      "|    4   N/A  N/A     41190      C   python                           2087MiB |\n",
      "|    4   N/A  N/A     87893      C   python                           2087MiB |\n",
      "|    4   N/A  N/A    216646      C   python                           2087MiB |\n",
      "|    4   N/A  N/A    253906      C   python                           2087MiB |\n",
      "|    5   N/A  N/A     43030      C   python                           2087MiB |\n",
      "|    5   N/A  N/A    127829      C   ...lin/miniconda3/bin/python     1689MiB |\n",
      "|    5   N/A  N/A    130398      C   python                           2087MiB |\n",
      "|    5   N/A  N/A    220998      C   python                           2087MiB |\n",
      "|    7   N/A  N/A      8620      C   ...ometric_turing/bin/python      685MiB |\n",
      "|    8   N/A  N/A    196477      C   python                          23555MiB |\n",
      "|    8   N/A  N/A    198831      C   python                           3553MiB |\n",
      "|    9   N/A  N/A    120164      C   ...lin/miniconda3/bin/python     1671MiB |\n",
      "|    9   N/A  N/A    122047      C   ...lin/miniconda3/bin/python     1671MiB |\n",
      "|    9   N/A  N/A    196478      C   python                          14459MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric_turing",
   "language": "python",
   "name": "geometric_turing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
