{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e39839a-9c02-4dda-996b-7e2211d4e74c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ura/miniconda3/envs/geometric/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# util \n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Callable, Tuple, Union\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing, NNConv, CGConv, GINEConv\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import reset, zeros\n",
    "from torch_geometric.typing import Adj, OptPairTensor, OptTensor, Size\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader, ClusterData, ClusterLoader\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "DATA_FOLDER = \"./data\"\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic=True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292c82b1-8709-487d-997c-1c47543e7551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_bigLITTLE_graph(data_folder, no_duplicate=False, unobserved=0.0, unobserved_edge=0.0):\n",
    "    list_files = os.listdir(data_folder)\n",
    "    list_files = list(filter(lambda x: x.endswith(\".pkl\"), list_files))\n",
    "    list_labels = pickle.load(open(\"labels.pkl\", \"rb\"))\n",
    "    \n",
    "    dict_graph_size = defaultdict(lambda: [])\n",
    "    set_cycle_size = set([])\n",
    "    set_branch_size = set([])\n",
    "    \n",
    "    # Directed or Undirected? Edge weights?\n",
    "    bigLITTLE_graph = nx.DiGraph()\n",
    "    gid = 0\n",
    "    \n",
    "    for gf in list_files:\n",
    "        idx, cycle_size, branch_size, _ = gf.split(\"_\")\n",
    "        cycle_size = int(cycle_size)\n",
    "        branch_size = int(branch_size)\n",
    "        \n",
    "        if no_duplicate and len(dict_graph_size[(cycle_size, branch_size)]) > 0:\n",
    "            continue\n",
    "            \n",
    "        # graph = nx.read_gpickle(os.path.join(DATA_FOLDER, gf))\n",
    "        \n",
    "        # For testing\n",
    "        # if cycle_size > 5 or branch_size > 2:\n",
    "        #     continue\n",
    "        \n",
    "        bigLITTLE_graph.add_node(gid, features=[float(cycle_size), float(branch_size)], label=list_labels[int(idx)])\n",
    "        dict_graph_size[(cycle_size, branch_size)].append(gid)\n",
    "        \n",
    "        set_cycle_size.add(cycle_size)\n",
    "        set_branch_size.add(branch_size)\n",
    "        gid += 1\n",
    "    # print(dict_graph_size)\n",
    "    # Undirected assumption?\n",
    "    # Same cycle_size & branch_size ==> Edge: [0,0]\n",
    "    # for k, items in dict_graph_size.items():\n",
    "    #     for item_idx1 in range(len(items)):\n",
    "    #         for item_idx2 in range(item_idx1+1, len(items)):\n",
    "    #             bigLITTLE_graph.add_edge(items[item_idx1], items[item_idx2], e=[0,0])\n",
    "                \n",
    "    # Same cycle size & Different branch size ==> Edge: [0, 1]\n",
    "    # Filter lists of same cycle_size\n",
    "    for cs in set_cycle_size:\n",
    "        list_same_cycle_size = list(filter(lambda x: x[0]==cs, dict_graph_size.keys()))\n",
    "        list_same_cycle_size = list(sorted(list_same_cycle_size, key=lambda x:x[1]))\n",
    "        # print(list_same_cycle_size)\n",
    "        for bs_idx1 in range(len(list_same_cycle_size)-1):\n",
    "            bs_idx2 = bs_idx1 + 1\n",
    "            key_cb1 = list_same_cycle_size[bs_idx1] # e.g. (3, 1)\n",
    "            key_cb2 = list_same_cycle_size[bs_idx2] # e.g. (3, 2)\n",
    "\n",
    "            for gid1 in dict_graph_size[key_cb1]:\n",
    "                for gid2 in dict_graph_size[key_cb2]:\n",
    "                    # print(key_cb2[1]-key_cb1[1])\n",
    "                    bigLITTLE_graph.add_edge(gid1, gid2, e=[0,1])\n",
    "                    bigLITTLE_graph.add_edge(gid2, gid1, e=[0,-1])\n",
    "                        \n",
    "    # Different cycle size & Same branch size ==> Edge: [1, 0]\n",
    "    for bs in set_branch_size:\n",
    "        list_same_branch_size = list(filter(lambda x: x[1]==bs, dict_graph_size.keys()))\n",
    "        list_same_branch_size = list(sorted(list_same_branch_size, key=lambda x:x[0]))\n",
    "        \n",
    "        for cs_idx1 in range(len(list_same_branch_size)-1):\n",
    "            cs_idx2 = cs_idx1 + 1\n",
    "            key_cb1 = list_same_branch_size[cs_idx1] # e.g. (3, 1)\n",
    "            key_cb2 = list_same_branch_size[cs_idx2] # e.g. (4, 1)\n",
    "\n",
    "            for gid1 in dict_graph_size[key_cb1]:\n",
    "                for gid2 in dict_graph_size[key_cb2]:\n",
    "                    bigLITTLE_graph.add_edge(gid1, gid2, e=[1,0])\n",
    "                    bigLITTLE_graph.add_edge(gid2, gid1, e=[-1,0])\n",
    "                    \n",
    "    \n",
    "    # Add all other edge as [0,0]\n",
    "    list_nodes = list(bigLITTLE_graph.nodes)\n",
    "    for i in range(len(list_nodes)):\n",
    "        for j in range(i, len(list_nodes)):\n",
    "            nid_i = list_nodes[i]\n",
    "            nid_j = list_nodes[j]\n",
    "            if not bigLITTLE_graph.has_edge(nid_i, nid_j):\n",
    "                bigLITTLE_graph.add_edge(gid1, gid2, e=[0,0])\n",
    "                bigLITTLE_graph.add_edge(gid2, gid1, e=[0,0])\n",
    "    \n",
    "    if unobserved > 0:\n",
    "        unobserved_node_idxs = np.random.choice(list(range(bigLITTLE_graph.number_of_nodes())), \n",
    "                                size=int(unobserved*bigLITTLE_graph.number_of_nodes()), \n",
    "                                replace=False)\n",
    "    else:\n",
    "        unobserved_node_idxs = None\n",
    "    \n",
    "    if unobserved_edge > 0:\n",
    "        # Remove unobserved edges\n",
    "        num_edge_to_remove = int(bigLITTLE_graph.number_of_edges() * unobserved_edge)\n",
    "            \n",
    "        list_edges = np.array([list(e) for e in bigLITTLE_graph.edges])\n",
    "        list_unique_edges = list_edges[list_edges[:,1] >= list_edges[:,0]]\n",
    "        edge_idxs = np.random.choice([0,1], \n",
    "                                size=list_unique_edges.shape[0], \n",
    "                                p=[unobserved_edge, 1-unobserved_edge])\n",
    "        list_remove_edges = list_unique_edges[edge_idxs==0]\n",
    "        \n",
    "        for edge in list_remove_edges:\n",
    "            u, v = edge\n",
    "            bigLITTLE_graph.remove_edge(u,v)\n",
    "            bigLITTLE_graph.remove_edge(v,u)\n",
    "    \n",
    "    return bigLITTLE_graph, unobserved_node_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb95bed-6236-4ec5-b354-92250f0b40bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_color(e):\n",
    "    if e == [0,1]:\n",
    "        return \"black\"\n",
    "    elif e == [0,-1]:\n",
    "        return \"red\"\n",
    "    elif e == [1,0]:\n",
    "        return \"green\"\n",
    "    elif e == [-1,0]:\n",
    "        return \"blue\"\n",
    "    else:\n",
    "        return \"yellow\"\n",
    "    \n",
    "def draw_graph(graph):\n",
    "    nodeLabels = {nid:graph.nodes[nid][\"label\"] for nid in graph.nodes}\n",
    "    nodeColors = \"grey\"\n",
    "    edgeColor = [get_edge_color(graph.edges[eid][\"e\"])for eid in graph.edges]\n",
    "\n",
    "    nx.draw(graph, nx.kamada_kawai_layout(graph), edge_color=edgeColor, width=1, linewidths=0.1,\n",
    "              node_size=500, node_color=nodeColors, alpha=0.9,\n",
    "              labels=nodeLabels)\n",
    "    \n",
    "def transform_func(graph):\n",
    "    graph.x = graph.x.to(\"cuda:0\")\n",
    "    graph.y = graph.label.to(\"cuda:0\")\n",
    "    graph.edge_attr = graph.edge_attr.to(\"cuda:0\")\n",
    "    graph.edge_index = graph.edge_index.to(\"cuda:0\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894466a0-867e-419f-8750-ccdc247b1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrickyNNConv(MessagePassing):\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, nn: Callable = None, aggr: str = 'add',\n",
    "                 root_weight: bool = True, bias: bool = True, **kwargs):\n",
    "        super().__init__(aggr=aggr, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.root_weight = root_weight\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.nn = Linear(out_channels, out_channels, bias=False, weight_initializer='uniform')\n",
    "        if root_weight:\n",
    "            self.lin = Linear(in_channels[1], out_channels, bias=False,\n",
    "                              weight_initializer='uniform')\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "        if self.root_weight:\n",
    "            self.lin.reset_parameters()\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        x_r = self.lin(x)\n",
    "\n",
    "        # propagate_type: (x: OptTensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x_r, edge_attr=edge_attr, size=size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        # weight = self.nn(edge_attr)\n",
    "        weight = edge_attr.view(-1, self.out_channels)\n",
    "        return self.nn(weight + x_j).squeeze(1) # torch.matmul(x_j.unsqueeze(1), weight).squeeze(1)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, aggr={self.aggr}, nn={self.nn})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19868cc-6dcf-4774-9fa1-c63b76b03f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, node_channels, edge_channels, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.edge_embed = nn.Linear(edge_channels, hidden_channels, bias=False)\n",
    "        \n",
    "        # self.node_embed = nn.Linear(node_channels, hidden_channels, bias=False)\n",
    "        # self.conv1 = CGConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.conv1 = TrickyNNConv(node_channels, hidden_channels, aggr=\"mean\")\n",
    "        self.conv1_e = nn.Linear(hidden_channels, hidden_channels, bias=False)\n",
    "        \n",
    "        self.conv2 = TrickyNNConv(hidden_channels, hidden_channels, aggr=\"mean\")\n",
    "        \n",
    "        # self.conv1 = GINEConv(\n",
    "        #     nn=nn.Sequential(nn.Linear(hidden_channels, node_channels*hidden_channels)),\n",
    "        #     edge_dim=hidden_channels\n",
    "        # )\n",
    "        \n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "        self.lin2 = nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "        self.lin3 = nn.Linear(hidden_channels, 1, bias=True)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \n",
    "        '''\n",
    "        edge_attr: batch_size * 2\n",
    "        '''\n",
    "        # 1. Obtain node embeddings\n",
    "        # x = self.node_embed(x)\n",
    "        e = self.edge_embed(edge_attr)\n",
    "        \n",
    "        z = self.conv1(x=x, edge_index=edge_index, edge_attr=e)\n",
    "        z = z.relu()\n",
    "        \n",
    "        e = self.conv1_e(e)#.relu()\n",
    "        z = self.conv2(x=z, edge_index=edge_index, edge_attr=e)\n",
    "        z = z.relu()\n",
    "        \n",
    "        # 2. Apply a final classifier\n",
    "        z = F.dropout(z, p=0.1, training=True)\n",
    "        \n",
    "        z = self.lin1(z)\n",
    "        z = z.relu()\n",
    "        z = self.lin2(z)\n",
    "        z = z.relu()\n",
    "        \n",
    "        z = self.lin3(z)\n",
    "        z = torch.sigmoid(z) * 110\n",
    "        \n",
    "        return z\n",
    "    \n",
    "def train(loader, random_mask=0, observed_idxs=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Iterate in batches over the training dataset\n",
    "    for data in loader:\n",
    "        # Random masking\n",
    "        if random_mask > 0:\n",
    "            idxs = np.random.choice(list(range(data.x.shape[0])), size=int(random_mask*data.x.shape[0]), replace=False)\n",
    "            data.x[idxs] = 1 - data.x[idxs]\n",
    "        \n",
    "        # Perform a single forward pass\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(\n",
    "            out[observed_idxs], \n",
    "            data.y[observed_idxs].view(-1, 1)\n",
    "        )\n",
    "        total_loss += loss\n",
    "        \n",
    "        loss.backward(); optimizer.step(); optimizer.zero_grad(); steps += 1\n",
    "\n",
    "    return total_loss / steps\n",
    "\n",
    "def test(loader, mc_dropout_sample=100):\n",
    "    model.eval()\n",
    "    mse = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Iterate in batches over the training/test dataset\n",
    "    for data in loader:\n",
    "        \n",
    "        out = []\n",
    "        for _ in range(mc_dropout_sample):\n",
    "            out.append(model(data.x, data.edge_index, data.edge_attr, data.batch))\n",
    "        out = torch.stack(out)\n",
    "        \n",
    "        # Check against ground-truth labels\n",
    "        mse += criterion(out.mean(0), data.y.view(-1, 1))\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        \n",
    "        # out_std = out.std(0)\n",
    "    return mse / steps  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5179f477-5d96-42c8-a22a-98a10994d99a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n",
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "generate_data = True\n",
    "unobserved_fraction = 0.2\n",
    "\n",
    "if generate_data:\n",
    "    bL_graph_train, unobserved_idxs = construct_bigLITTLE_graph(DATA_FOLDER, unobserved=unobserved_fraction)\n",
    "    # draw_graph(bL_graph_train)\n",
    "    \n",
    "    data_train = from_networkx(bL_graph_train)\n",
    "    data_train.x = torch.ones(len(data_train.g), 2).type(torch.FloatTensor)\n",
    "    data_train.x[:,1] = 0\n",
    "    \n",
    "    # data_train.x = np.random.choice([0,1], size=data_train.num_nodes, p=[0.2, 0.8])\n",
    "    # x_not = np.logical_not(data_train.x)\n",
    "    # data_train.x = np.stack([data_train.x, x_not], axis=-1)\n",
    "    # data_train.x = torch.tensor(data_train.x).type(torch.FloatTensor)\n",
    "    \n",
    "    data_train.edge_attr = data_train.e.type(torch.FloatTensor)\n",
    "    # data_train.train_mask = np.ones(data_train.num_nodes) \n",
    "    # data.train_mask = np.random.choice(\n",
    "    #     [0, 1], size=data.num_nodes, p=[0.2, 0.8])\n",
    "    # data_train.test_mask = np.logical_not(data_train.train_mask)\n",
    "    data_train = transform_func(data_train)\n",
    "    \n",
    "    c_data_train = ClusterData(data_train, num_parts=1, recursive=True)\n",
    "    train_loader = ClusterLoader(c_data_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    bL_graph_test = construct_bigLITTLE_graph(DATA_FOLDER)\n",
    "    # draw_graph(bL_graph)\n",
    "    \n",
    "    data_test = from_networkx(bL_graph_test)\n",
    "    data_test.x = torch.ones(len(data_test.g), 2).type(torch.FloatTensor)\n",
    "    data_test.x[:,1] = 0\n",
    "    data_test.edge_attr = data_test.e.type(torch.FloatTensor)\n",
    "    data_test = transform_func(data_test)\n",
    "    \n",
    "    c_data_test = ClusterData(data_test, num_parts=1, recursive=True)\n",
    "    test_loader = ClusterLoader(c_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b9da8e-1386-4ae5-84f1-4ceb2a9084a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw_graph(to_networkx(next(iter(train_loader)), node_attrs=[\"label\"], edge_attrs=[\"e\"]))\n",
    "bL_graph_train.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd4a9981-e12b-486a-8f9d-f142bbf84cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "neighbors_list = []\n",
    "for nid in bL_graph_train.nodes:\n",
    "    node_neighbors = bL_graph_train.neighbors(nid)\n",
    "    node_observed_neighbors = set(node_neighbors) - set(unobserved_idxs)\n",
    "    node_unobserved_neighbors = set(node_neighbors) - set(node_observed_neighbors)\n",
    "    neighbors_list.append([nid, len(node_observed_neighbors), len(node_unobserved_neighbors)])\n",
    "# print(neighbors_list)\n",
    "neighbors_list = np.array(neighbors_list)\n",
    "print(np.where(neighbors_list[:,1] == 0))\n",
    "assert np.where(neighbors_list[:,1] == 0)[0].shape[0] == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f0b98c6-b51f-45fe-98eb-f601af591645",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1028/100000 [00:13<21:17, 77.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000, Train MAE: 0.5517, Test MAE: 3.6134, Min MAE: 3.6134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1479/100000 [00:19<21:24, 76.72it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m observed_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(bL_graph_train\u001b[38;5;241m.\u001b[39mnodes) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(unobserved_idxs))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 21\u001b[0m     train_mse \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved_idxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved_idxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m         clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [5], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(loader, random_mask, observed_idxs)\u001b[0m\n\u001b[1;32m     56\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Iterate in batches over the training dataset\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Random masking\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random_mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     62\u001b[0m         idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(random_mask\u001b[38;5;241m*\u001b[39mdata\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/geometric/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/geometric/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/geometric/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/geometric/lib/python3.8/site-packages/torch_geometric/loader/cluster.py:147\u001b[0m, in \u001b[0;36mClusterLoader.__collate__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    144\u001b[0m E \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_data\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnum_edges\n\u001b[1;32m    146\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_data\u001b[38;5;241m.\u001b[39mpartptr[batch]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m--> 147\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartptr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    148\u001b[0m node_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39marange(s, e) \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(start, end)])\n\u001b[1;32m    150\u001b[0m data \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_data\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_mse = 1e10\n",
    "min_epoch = 0\n",
    "epochs = 100000\n",
    "lr = 0.0005\n",
    "device = \"cuda:0\"\n",
    "hidden_channels = 64\n",
    "\n",
    "model = GNN(node_channels=2, edge_channels=2, \n",
    "            hidden_channels=hidden_channels).to(device)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(model)\n",
    "print(\"Number of parameters: \", params)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "observed_idxs = list(set(bL_graph_train.nodes) - set(unobserved_idxs))\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_mse = train(train_loader, random_mask=0, observed_idxs=observed_idxs)\n",
    "    \n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        clear_output(wait=True)\n",
    "        test_mse = test(test_loader)\n",
    "        if test_mse < min_mse:\n",
    "            min_mse = test_mse\n",
    "            min_epoch = epoch\n",
    "        print(f'Epoch: {epoch+1:03d}, Train MAE: {train_mse:.4f},',\n",
    "              f'Test MAE: {test_mse:.4f}, Min MAE: {min_mse:.4f}')\n",
    "    else: test_mse = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1224f13-1cbe-4885-ac5b-8b3ce0f3f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 10 neighbors for each node for 2 iterations\n",
    "    num_neighbors=[10],\n",
    "    # Use a batch size of 128 for sampling training nodes\n",
    "    batch_size=128,\n",
    "    input_nodes=data.train_mask\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 10 neighbors for each node for 2 iterations\n",
    "    num_neighbors=[10],\n",
    "    # Use a batch size of 128 for sampling training nodes\n",
    "    batch_size=128,\n",
    "    input_nodes=data.test_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25f2e2-f848-45f8-bec7-2fd40207d212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0a2a8-7378-4bc7-aa38-5b432b43ccbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e91e1e-301e-4ab7-93f0-f725e3732fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed node and masked nodes \n",
    "\n",
    "\n",
    "# obs_answers_id = [ np.concatenate([\n",
    "#     # np.array([True]),\n",
    "#     np.random.choice([True]*(allow)+ [False]*(n_questions-allow), (n_questions), replace=False), \\\n",
    "#     # np.random.choice([True]*1 + [False]*(n_treatments-1), (n_treatments), replace=False) \\\n",
    "#     ]) for _ in range(n_sample)]\n",
    "\n",
    "# obs_answers_id = np.stack(obs_answers_id)\n",
    "# obs_answers_id = torch.tensor(obs_answers_id)\n",
    "\n",
    "# obs_outcomes_id = [\n",
    "#     np.random.choice([True]*1 + [False]*(n_treatments-1), (n_treatments), replace=False) \\\n",
    "#     for _ in range(n_sample)]\n",
    "# obs_outcomes_id = np.stack(obs_outcomes_id)\n",
    "# obs_outcomes_id = torch.tensor(obs_outcomes_id)\n",
    "\n",
    "# mask_obs_answers_id = copy.deepcopy(obs_answers_id)\n",
    "# for sample in range(n_sample):\n",
    "#     all_true_id = torch.where(obs_answers_id[sample])[0]\n",
    "#     flip = np.random.choice(all_true_id)\n",
    "#     mask_obs_answers_id[sample, flip] = False\n",
    "\n",
    "# obs_answers = copy.deepcopy(answers)\n",
    "# obs_answers[torch.logical_not(obs_answers_id)] = torch.tensor([0, 1], device=device).double()\n",
    "\n",
    "# mask_obs_answers = copy.deepcopy(answers)\n",
    "# mask_obs_answers[torch.logical_not(mask_obs_answers_id)] = torch.tensor([0, 1], device=device).double()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "geometric"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
