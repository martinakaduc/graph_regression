{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# util \n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Callable, Tuple, Union\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing, NNConv, CGConv, GINEConv\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import reset, zeros\n",
    "from torch_geometric.typing import Adj, OptPairTensor, OptTensor, Size\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader, ClusterData, ClusterLoader\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "DATA_FOLDER = \"./data\"\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic=True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_bigLITTLE_graph(data_folder, no_duplicate=False, unobserved=0.0, unobserved_edge=0.0):\n",
    "    list_files = os.listdir(data_folder)\n",
    "    list_files = list(filter(lambda x: x.endswith(\".pkl\"), list_files))\n",
    "    list_labels = pickle.load(open(\"labels.pkl\", \"rb\"))\n",
    "    \n",
    "    dict_graph_size = defaultdict(lambda: [])\n",
    "    set_cycle_size = set([])\n",
    "    set_branch_size = set([])\n",
    "    \n",
    "    # Directed or Undirected? Edge weights?\n",
    "    bigLITTLE_graph = nx.DiGraph()\n",
    "    gid = 0\n",
    "    \n",
    "    for gf in list_files:\n",
    "        idx, cycle_size, branch_size, _ = gf.split(\"_\")\n",
    "        cycle_size = int(cycle_size)\n",
    "        branch_size = int(branch_size)\n",
    "        \n",
    "        if no_duplicate and len(dict_graph_size[(cycle_size, branch_size)]) > 0:\n",
    "            continue\n",
    "            \n",
    "        # graph = nx.read_gpickle(os.path.join(DATA_FOLDER, gf))\n",
    "        \n",
    "        # For testing\n",
    "        # if cycle_size > 5 or branch_size > 2:\n",
    "        #     continue\n",
    "        \n",
    "        bigLITTLE_graph.add_node(gid, features=[float(cycle_size), float(branch_size)], label=list_labels[int(idx)])\n",
    "        dict_graph_size[(cycle_size, branch_size)].append(gid)\n",
    "        \n",
    "        set_cycle_size.add(cycle_size)\n",
    "        set_branch_size.add(branch_size)\n",
    "        gid += 1\n",
    "    # print(dict_graph_size)\n",
    "    # Undirected assumption?\n",
    "    # Same cycle_size & branch_size ==> Edge: [0,0]\n",
    "    # for k, items in dict_graph_size.items():\n",
    "    #     for item_idx1 in range(len(items)):\n",
    "    #         for item_idx2 in range(item_idx1+1, len(items)):\n",
    "    #             bigLITTLE_graph.add_edge(items[item_idx1], items[item_idx2], e=[0,0])\n",
    "                \n",
    "    # Same cycle size & Different branch size ==> Edge: [0, 1]\n",
    "    # Filter lists of same cycle_size\n",
    "    for cs in set_cycle_size:\n",
    "        list_same_cycle_size = list(filter(lambda x: x[0]==cs, dict_graph_size.keys()))\n",
    "        list_same_cycle_size = list(sorted(list_same_cycle_size, key=lambda x:x[1]))\n",
    "        # print(list_same_cycle_size)\n",
    "        for bs_idx1 in range(len(list_same_cycle_size)-1):\n",
    "            bs_idx2 = bs_idx1 + 1\n",
    "            key_cb1 = list_same_cycle_size[bs_idx1] # e.g. (3, 1)\n",
    "            key_cb2 = list_same_cycle_size[bs_idx2] # e.g. (3, 2)\n",
    "\n",
    "            for gid1 in dict_graph_size[key_cb1]:\n",
    "                for gid2 in dict_graph_size[key_cb2]:\n",
    "                    # print(key_cb2[1]-key_cb1[1])\n",
    "                    bigLITTLE_graph.add_edge(gid1, gid2, e=[0,1])\n",
    "                    bigLITTLE_graph.add_edge(gid2, gid1, e=[0,-1])\n",
    "                        \n",
    "    # Different cycle size & Same branch size ==> Edge: [1, 0]\n",
    "    for bs in set_branch_size:\n",
    "        list_same_branch_size = list(filter(lambda x: x[1]==bs, dict_graph_size.keys()))\n",
    "        list_same_branch_size = list(sorted(list_same_branch_size, key=lambda x:x[0]))\n",
    "        \n",
    "        for cs_idx1 in range(len(list_same_branch_size)-1):\n",
    "            cs_idx2 = cs_idx1 + 1\n",
    "            key_cb1 = list_same_branch_size[cs_idx1] # e.g. (3, 1)\n",
    "            key_cb2 = list_same_branch_size[cs_idx2] # e.g. (4, 1)\n",
    "\n",
    "            for gid1 in dict_graph_size[key_cb1]:\n",
    "                for gid2 in dict_graph_size[key_cb2]:\n",
    "                    bigLITTLE_graph.add_edge(gid1, gid2, e=[1,0])\n",
    "                    bigLITTLE_graph.add_edge(gid2, gid1, e=[-1,0])\n",
    "                    \n",
    "    \n",
    "    # Add all other edge as [0,0]\n",
    "    list_nodes = list(bigLITTLE_graph.nodes)\n",
    "    for i in range(len(list_nodes)):\n",
    "        for j in range(i, len(list_nodes)):\n",
    "            nid_i = list_nodes[i]\n",
    "            nid_j = list_nodes[j]\n",
    "            if not bigLITTLE_graph.has_edge(nid_i, nid_j):\n",
    "                bigLITTLE_graph.add_edge(gid1, gid2, e=[0,0])\n",
    "                bigLITTLE_graph.add_edge(gid2, gid1, e=[0,0])\n",
    "    \n",
    "    if unobserved > 0:\n",
    "        unobserved_node_idxs = np.random.choice(list(range(bigLITTLE_graph.number_of_nodes())), \n",
    "                                size=int(unobserved*bigLITTLE_graph.number_of_nodes()), \n",
    "                                replace=False)\n",
    "    else:\n",
    "        unobserved_node_idxs = None\n",
    "    \n",
    "    if unobserved_edge > 0:\n",
    "        # Remove unobserved edges\n",
    "        num_edge_to_remove = int(bigLITTLE_graph.number_of_edges() * unobserved_edge)\n",
    "            \n",
    "        list_edges = np.array([list(e) for e in bigLITTLE_graph.edges])\n",
    "        list_unique_edges = list_edges[list_edges[:,1] >= list_edges[:,0]]\n",
    "        edge_idxs = np.random.choice([0,1], \n",
    "                                size=list_unique_edges.shape[0], \n",
    "                                p=[unobserved_edge, 1-unobserved_edge])\n",
    "        list_remove_edges = list_unique_edges[edge_idxs==0]\n",
    "        \n",
    "        for edge in list_remove_edges:\n",
    "            u, v = edge\n",
    "            bigLITTLE_graph.remove_edge(u,v)\n",
    "            bigLITTLE_graph.remove_edge(v,u)\n",
    "    \n",
    "    return bigLITTLE_graph, unobserved_node_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_edge_color(e):\n",
    "    if e == [0,1]:\n",
    "        return \"black\"\n",
    "    elif e == [0,-1]:\n",
    "        return \"red\"\n",
    "    elif e == [1,0]:\n",
    "        return \"green\"\n",
    "    elif e == [-1,0]:\n",
    "        return \"blue\"\n",
    "    else:\n",
    "        return \"yellow\"\n",
    "    \n",
    "def draw_graph(graph):\n",
    "    nodeLabels = {nid:graph.nodes[nid][\"label\"] for nid in graph.nodes}\n",
    "    nodeColors = \"grey\"\n",
    "    edgeColor = [get_edge_color(graph.edges[eid][\"e\"])for eid in graph.edges]\n",
    "\n",
    "    nx.draw(graph, nx.kamada_kawai_layout(graph), edge_color=edgeColor, width=1, linewidths=0.1,\n",
    "              node_size=500, node_color=nodeColors, alpha=0.9,\n",
    "              labels=nodeLabels)\n",
    "    \n",
    "def transform_func(graph):\n",
    "    graph.x = graph.x.to(\"cuda:0\")\n",
    "    graph.y = graph.label.to(\"cuda:0\")\n",
    "    graph.edge_attr = graph.edge_attr.to(\"cuda:0\")\n",
    "    graph.edge_index = graph.edge_index.to(\"cuda:0\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrickyNNConv(MessagePassing):\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, nn: Callable = None, aggr: str = 'add',\n",
    "                 root_weight: bool = True, bias: bool = True, **kwargs):\n",
    "        super().__init__(aggr=aggr, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.root_weight = root_weight\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.nn = Linear(out_channels, out_channels, bias=False, weight_initializer='uniform')\n",
    "        if root_weight:\n",
    "            self.lin = Linear(in_channels[1], out_channels, bias=False,\n",
    "                              weight_initializer='uniform')\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "        if self.root_weight:\n",
    "            self.lin.reset_parameters()\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        x_r = self.lin(x)\n",
    "\n",
    "        # propagate_type: (x: OptTensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x_r, edge_attr=edge_attr, size=size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        # weight = self.nn(edge_attr)\n",
    "        weight = edge_attr.view(-1, self.out_channels)\n",
    "        return self.nn(weight + x_j).squeeze(1) # torch.matmul(x_j.unsqueeze(1), weight).squeeze(1)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, aggr={self.aggr}, nn={self.nn})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, node_channels, edge_channels, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.edge_embed = nn.Linear(edge_channels, hidden_channels, bias=False)\n",
    "        \n",
    "        # self.node_embed = nn.Linear(node_channels, hidden_channels, bias=False)\n",
    "        # self.conv1 = CGConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.conv1 = TrickyNNConv(node_channels, hidden_channels, aggr=\"mean\")\n",
    "        self.conv1_e = nn.Linear(hidden_channels, hidden_channels, bias=False)\n",
    "        \n",
    "        self.conv2 = TrickyNNConv(hidden_channels, hidden_channels, aggr=\"mean\")\n",
    "        \n",
    "        # self.conv1 = GINEConv(\n",
    "        #     nn=nn.Sequential(nn.Linear(hidden_channels, node_channels*hidden_channels)),\n",
    "        #     edge_dim=hidden_channels\n",
    "        # )\n",
    "        \n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "        self.lin2 = nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "        self.lin3 = nn.Linear(hidden_channels, 1, bias=True)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \n",
    "        '''\n",
    "        edge_attr: batch_size * 2\n",
    "        '''\n",
    "        # 1. Obtain node embeddings\n",
    "        # x = self.node_embed(x)\n",
    "        e = self.edge_embed(edge_attr)\n",
    "        \n",
    "        z = self.conv1(x=x, edge_index=edge_index, edge_attr=e)\n",
    "        z = z.relu()\n",
    "        \n",
    "        e = self.conv1_e(e)#.relu()\n",
    "        z = self.conv2(x=z, edge_index=edge_index, edge_attr=e)\n",
    "        z = z.relu()\n",
    "        \n",
    "        # 2. Apply a final classifier\n",
    "        z = F.dropout(z, p=0.1, training=True)\n",
    "        \n",
    "        z = self.lin1(z)\n",
    "        z = z.relu()\n",
    "        z = self.lin2(z)\n",
    "        z = z.relu()\n",
    "        \n",
    "        z = self.lin3(z)\n",
    "        z = torch.sigmoid(z) * 110\n",
    "        \n",
    "        return z\n",
    "    \n",
    "def train(loader, random_mask=0, observed_idxs=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Iterate in batches over the training dataset\n",
    "    for data in loader:\n",
    "        # Random masking\n",
    "        if random_mask > 0:\n",
    "            idxs = np.random.choice(list(range(data.x.shape[0])), size=int(random_mask*data.x.shape[0]), replace=False)\n",
    "            data.x[idxs] = 1 - data.x[idxs]\n",
    "        \n",
    "        # Perform a single forward pass\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(\n",
    "            out[observed_idxs], \n",
    "            data.y[observed_idxs].view(-1, 1)\n",
    "        )\n",
    "        total_loss += loss\n",
    "        \n",
    "        loss.backward(); optimizer.step(); optimizer.zero_grad(); steps += 1\n",
    "\n",
    "    return total_loss / steps\n",
    "\n",
    "def test(loader, mc_dropout_sample=100):\n",
    "    model.eval()\n",
    "    mse = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Iterate in batches over the training/test dataset\n",
    "    for data in loader:\n",
    "        \n",
    "        out = []\n",
    "        for _ in range(mc_dropout_sample):\n",
    "            out.append(model(data.x, data.edge_index, data.edge_attr, data.batch))\n",
    "        out = torch.stack(out)\n",
    "        \n",
    "        # Check against ground-truth labels\n",
    "        mse += criterion(out.mean(0), data.y.view(-1, 1))\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        \n",
    "        # out_std = out.std(0)\n",
    "    return mse / steps  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n",
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "generate_data = True\n",
    "unobserved_fraction = 0.2\n",
    "\n",
    "if generate_data:\n",
    "    bL_graph_train, unobserved_idxs = construct_bigLITTLE_graph(DATA_FOLDER, unobserved=unobserved_fraction)\n",
    "    # draw_graph(bL_graph_train)\n",
    "    \n",
    "    data_train = from_networkx(bL_graph_train)\n",
    "    data_train.x = torch.ones(len(data_train.label), 2).type(torch.FloatTensor)\n",
    "    data_train.x[:,1] = 0\n",
    "    \n",
    "    # data_train.x = np.random.choice([0,1], size=data_train.num_nodes, p=[0.2, 0.8])\n",
    "    # x_not = np.logical_not(data_train.x)\n",
    "    # data_train.x = np.stack([data_train.x, x_not], axis=-1)\n",
    "    # data_train.x = torch.tensor(data_train.x).type(torch.FloatTensor)\n",
    "    \n",
    "    data_train.edge_attr = data_train.e.type(torch.FloatTensor)\n",
    "    # data_train.train_mask = np.ones(data_train.num_nodes) \n",
    "    # data.train_mask = np.random.choice(\n",
    "    #     [0, 1], size=data.num_nodes, p=[0.2, 0.8])\n",
    "    # data_train.test_mask = np.logical_not(data_train.train_mask)\n",
    "    data_train = transform_func(data_train)\n",
    "    \n",
    "    c_data_train = ClusterData(data_train, num_parts=1, recursive=True)\n",
    "    train_loader = ClusterLoader(c_data_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    bL_graph_test, _ = construct_bigLITTLE_graph(DATA_FOLDER)\n",
    "    # draw_graph(bL_graph)\n",
    "    \n",
    "    data_test = from_networkx(bL_graph_test)\n",
    "    data_test.x = torch.ones(len(data_test.label), 2).type(torch.FloatTensor)\n",
    "    data_test.x[:,1] = 0\n",
    "    data_test.edge_attr = data_test.e.type(torch.FloatTensor)\n",
    "    data_test = transform_func(data_test)\n",
    "    \n",
    "    c_data_test = ClusterData(data_test, num_parts=1, recursive=True)\n",
    "    test_loader = ClusterLoader(c_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw_graph(to_networkx(next(iter(train_loader)), node_attrs=[\"label\"], edge_attrs=[\"e\"]))\n",
    "bL_graph_train.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "neighbors_list = []\n",
    "for nid in bL_graph_train.nodes:\n",
    "    node_neighbors = bL_graph_train.neighbors(nid)\n",
    "    node_observed_neighbors = set(node_neighbors) - set(unobserved_idxs)\n",
    "    node_unobserved_neighbors = set(node_neighbors) - set(node_observed_neighbors)\n",
    "    neighbors_list.append([nid, len(node_observed_neighbors), len(node_unobserved_neighbors)])\n",
    "# print(neighbors_list)\n",
    "neighbors_list = np.array(neighbors_list)\n",
    "print(np.where(neighbors_list[:,1] == 0))\n",
    "assert np.where(neighbors_list[:,1] == 0)[0].shape[0] == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (edge_embed): Linear(in_features=2, out_features=64, bias=False)\n",
      "  (conv1): TrickyNNConv(2, 64, aggr=mean, nn=Linear(64, 64, bias=False))\n",
      "  (conv1_e): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (conv2): TrickyNNConv(64, 64, aggr=mean, nn=Linear(64, 64, bias=False))\n",
      "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  25153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▌                                                                                                                                      | 458/100000 [00:22<1:19:57, 20.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_64273/2170779130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserved_idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_64273/41692345.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, random_mask, observed_idxs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Iterate in batches over the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Random masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom_mask\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/sttruong/geometric_turing/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/sttruong/geometric_turing/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/sttruong/geometric_turing/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/dfs/user/sttruong/geometric_turing/lib/python3.7/site-packages/torch_geometric/loader/cluster.py\u001b[0m in \u001b[0;36m__collate__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_mse = 1e10\n",
    "min_epoch = 0\n",
    "epochs = 100000\n",
    "lr = 0.0005\n",
    "device = \"cuda:0\"\n",
    "hidden_channels = 64\n",
    "\n",
    "model = GNN(node_channels=2, edge_channels=2, \n",
    "            hidden_channels=hidden_channels).to(device)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(model)\n",
    "print(\"Number of parameters: \", params)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "observed_idxs = list(set(bL_graph_train.nodes) - set(unobserved_idxs))\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_mse = train(train_loader, random_mask=0, observed_idxs=observed_idxs)\n",
    "    \n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        clear_output(wait=True)\n",
    "        test_mse = test(test_loader)\n",
    "        if test_mse < min_mse:\n",
    "            min_mse = test_mse\n",
    "            min_epoch = epoch\n",
    "        print(f'Epoch: {epoch+1:03d}, Train MAE: {train_mse:.4f},',\n",
    "              f'Test MAE: {test_mse:.4f}, Min MAE: {min_mse:.4f}')\n",
    "    else: test_mse = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 10 neighbors for each node for 2 iterations\n",
    "    num_neighbors=[10],\n",
    "    # Use a batch size of 128 for sampling training nodes\n",
    "    batch_size=128,\n",
    "    input_nodes=data.train_mask\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 10 neighbors for each node for 2 iterations\n",
    "    num_neighbors=[10],\n",
    "    # Use a batch size of 128 for sampling training nodes\n",
    "    batch_size=128,\n",
    "    input_nodes=data.test_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed node and masked nodes \n",
    "\n",
    "\n",
    "# obs_answers_id = [ np.concatenate([\n",
    "#     # np.array([True]),\n",
    "#     np.random.choice([True]*(allow)+ [False]*(n_questions-allow), (n_questions), replace=False), \\\n",
    "#     # np.random.choice([True]*1 + [False]*(n_treatments-1), (n_treatments), replace=False) \\\n",
    "#     ]) for _ in range(n_sample)]\n",
    "\n",
    "# obs_answers_id = np.stack(obs_answers_id)\n",
    "# obs_answers_id = torch.tensor(obs_answers_id)\n",
    "\n",
    "# obs_outcomes_id = [\n",
    "#     np.random.choice([True]*1 + [False]*(n_treatments-1), (n_treatments), replace=False) \\\n",
    "#     for _ in range(n_sample)]\n",
    "# obs_outcomes_id = np.stack(obs_outcomes_id)\n",
    "# obs_outcomes_id = torch.tensor(obs_outcomes_id)\n",
    "\n",
    "# mask_obs_answers_id = copy.deepcopy(obs_answers_id)\n",
    "# for sample in range(n_sample):\n",
    "#     all_true_id = torch.where(obs_answers_id[sample])[0]\n",
    "#     flip = np.random.choice(all_true_id)\n",
    "#     mask_obs_answers_id[sample, flip] = False\n",
    "\n",
    "# obs_answers = copy.deepcopy(answers)\n",
    "# obs_answers[torch.logical_not(obs_answers_id)] = torch.tensor([0, 1], device=device).double()\n",
    "\n",
    "# mask_obs_answers = copy.deepcopy(answers)\n",
    "# mask_obs_answers[torch.logical_not(mask_obs_answers_id)] = torch.tensor([0, 1], device=device).double()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric_turing",
   "language": "python",
   "name": "geometric_turing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
